{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 - Identificando personas desde una imagen de su cara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborado por GRUPO 1:\n",
    "\n",
    "- Juanita Piraban Barbosa - 201216313\n",
    "- Lorena Morales Rodríguez - 202027957\n",
    "- Alejandro Barinas Guio - 201628859\n",
    "- Jaime Humberto Trujillo Perea - 201920366\n",
    "- Alexander Zapata Galindo - 201425426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,ZeroPadding2D,Dropout,Softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.python.keras import Sequential\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.datasets import fetch_lfw_pairs\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from keras import backend as K\n",
    "from livelossplot import PlotLossesKeras\n",
    "# others\n",
    "from random import randrange\n",
    "from time import time\n",
    "# hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet : “Labeled Faces in the Wild”\n",
    "min_faces_per_person = 60 # número mínimo de fotos por personaje para filtrar la base\n",
    "resize=0.5 # Número de define la calidad de las imágenes utilizar 0.5 y 0.7\n",
    "\n",
    "# con 10 mín  fotos se generan 158 categorías\n",
    "# con 20 mín  fotos se generan 62 categorías\n",
    "# con 40 mín  fotos se generan 19 categorías\n",
    "# con 50 mín  fotos se generan 12 categorías\n",
    "# Probar y comparar 20, 40 y 70 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen Imágenes a procesar\n",
      "Samples: 1348\n",
      "Classes: 8\n",
      "Dimentions: (62, 47)\n"
     ]
    }
   ],
   "source": [
    "# Some definitions\n",
    "\n",
    "data = fetch_lfw_people(min_faces_per_person=min_faces_per_person, resize=resize)\n",
    "\n",
    "l=list(data.keys())\n",
    "X=data[l[0]]\n",
    "y=data[l[2]]\n",
    "target_names=data[l[3]]\n",
    "\n",
    "_,W,H=data[l[1]].shape\n",
    "features=X.shape[1]\n",
    "m=X.shape[0]\n",
    "classes=data[l[3]].shape[0]\n",
    "\n",
    "print(\"Resumen Imágenes a procesar\")\n",
    "print(\"Samples:\",m)\n",
    "print(\"Classes:\",classes)\n",
    "print(\"Dimentions:\",(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "\n",
    "temp=list(zip(X,y)) \n",
    "random.shuffle(temp) \n",
    "X,y=zip(*temp)\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splitting of data\n",
    "training_data_X,testing_data_X,training_data_Y,testing_data_Y=train_test_split(X,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizar y redimensionar bases\n",
    "training_data_Y=to_categorical(training_data_Y)\n",
    "testing_data_Y=to_categorical(testing_data_Y)\n",
    "\n",
    "training_data_X=training_data_X.reshape(training_data_X.shape[0],W,H,1)\n",
    "testing_data_X=testing_data_X.reshape(testing_data_X.shape[0],W,H,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO 5 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "K.clear_session()\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu',input_shape=(W,H,1)))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(Conv2D(128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(Conv2D(256,kernel_size=5,activation='relu'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes,activation='softmax'))\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters\n",
    "size = 128\n",
    "epoc = 50 # Número de épocas\n",
    "\n",
    "\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "model.fit(training_data_X,training_data_Y,validation_data=(testing_data_X,testing_data_Y),batch_size=size,\\\n",
    "          epochs=epoc, verbose=1, callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's performance on test set\n",
    "test_loss, test_acc = model.evaluate(testing_data_X,testing_data_Y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the model for Keras wrapper to scikit learn\n",
    "# we will optimize the type of pooling layer (max or average) and the activation function of the 2nd and 3rd convolution layers \n",
    "def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.10, optimizer = 'adam'):\n",
    "    # create model\n",
    "    # CNN model\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer: convolution\n",
    "    model.add(Conv2D(64,kernel_size=3,activation='relu',input_shape=(W,H,1)))\n",
    "    if pool_type == 'max':\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == 'average':\n",
    "        model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # second series of layers: convolution, pooling\n",
    "    model.add(Conv2D(128,kernel_size=5,activation='relu'))\n",
    "    if pool_type == 'max':\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == 'average':\n",
    "        model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # third series of layers: convolution, pooling, and dropout    \n",
    "    model.add(Conv2D(256,kernel_size=5,activation='relu'))\n",
    "    if pool_type == 'max':\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == 'average':\n",
    "        model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Dropout\n",
    "    model.add(Dropout(rate=dropout_rate))     \n",
    "    # fourth series\n",
    "    model.add(Flatten())         \n",
    "    \n",
    "    # MLP, ouput layer\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually test function hyperparameters\n",
    "#size = 128\n",
    "#epoc = 50\n",
    "#\n",
    "#cnn = create_cnn_model()\n",
    "#\n",
    "#cnn.compile(\n",
    "#  optimizer=optimizer,\n",
    "#  loss='categorical_crossentropy',  \n",
    "#  metrics=['accuracy'],\n",
    "#)\n",
    "#\n",
    "#cnn.summary()\n",
    "#\n",
    "#cnn.fit(training_data_X,\n",
    "#                  training_data_Y,\n",
    "#                  validation_data=(testing_data_X,testing_data_Y),\n",
    "#                  batch_size=size,\n",
    "#                  epochs=epoc, \n",
    "#                  verbose=1, \n",
    "#                  callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters tunning\n",
    "\n",
    "model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
    "n_cv = 2\n",
    "\n",
    "batch_size_opt= [128] \n",
    "epochs_opt= [2]\n",
    "optimizers_opt = ['adam', 'rmsprop','adamax']\n",
    "dropout_opt = [0.1,0.25,0.35,0.5]\n",
    "conv_activation_opt = ['relu','sigmoid']\n",
    "pool_type_opt  = ['max', 'average']\n",
    "\n",
    "parameters_opt = {\n",
    "                  'pool_type': pool_type_opt,\n",
    "                  'conv_activation': conv_activation_opt,    \n",
    "                  'dropout_rate' : dropout_opt,\n",
    "                  'optimizer': optimizers_opt,\n",
    "                  'batch_size': batch_size_opt, \n",
    "                  'epochs': epochs_opt,      \n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid =parameters_opt, \n",
    "                           n_jobs=-1,\n",
    "                           cv = n_cv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(training_data_X, training_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(best_parameters)\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIBLIOGRAFÍA\n",
    "\n",
    "https://www.kaggle.com/joparga3/2-tuning-parameters-for-logistic-regression\n",
    "\n",
    "https://www.projectpro.io/recipes/optimize-hyper-parameters-of-logistic-regression-model-using-grid-search-in-python\n",
    "\n",
    "https://github.com/harshitrai17152/Labeled-Faces-in-the-Wild.git\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 - Random Forest - Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborado por GRUPO 1:\n",
    "\n",
    "- Juanita Piraban Barbosa - 201216313\n",
    "- Lorena Morales Rodríguez - 202027957\n",
    "- Alejandro Barinas Guio - 201628859\n",
    "- Jaime Humberto Trujillo Perea - 201920366\n",
    "- Alexander Zapata Galindo - 201425426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Price Prediction\n",
    "\n",
    "Predict if the price of a car is low or high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "      <th>HighPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016</td>\n",
       "      <td>29242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2015</td>\n",
       "      <td>26465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2012</td>\n",
       "      <td>46739</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2017</td>\n",
       "      <td>41722</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2014</td>\n",
       "      <td>77669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL  M_CamryLE  \\\n",
       "15   2016    29242        0           0            0         0          1   \n",
       "47   2015    26465        0           0            0         0          1   \n",
       "85   2012    46739        0           1            0         0          0   \n",
       "141  2017    41722        0           0            0         0          0   \n",
       "226  2014    77669        0           0            0         0          0   \n",
       "\n",
       "     M_CamrySE  M_CamryXLE  HighPrice  \n",
       "15           0           0          1  \n",
       "47           0           0          1  \n",
       "85           0           0          1  \n",
       "141          1           0          1  \n",
       "226          0           1          0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Dataset/dataTraincarListings.csv')\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data['HighPrice'] = (data['Price'] > data['Price'].mean()).astype(int)\n",
    "data = data.drop(['Model', 'Price'], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13150, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['HighPrice']\n",
    "X = data.drop(['HighPrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.1\n",
    "\n",
    "Estimate a Decision Tree Classifier Manually using the code created in the Notebook #13\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.2\n",
    "\n",
    "Estimate a Bagging of 10 Decision Tree Classifiers Manually using the code created in the Notebook #7\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.3\n",
    "\n",
    "Implement the variable max_features on the Decision Tree Classifier created in 11.1.\n",
    "\n",
    "Compare the impact in the results by varing the parameter max_features\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.4\n",
    "\n",
    "Estimate a Bagging of 10 Decision Tree Classifiers with `max_features = log(n_features)`\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.5\n",
    "\n",
    "Using sklearn, train a RandomForestClassifier\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import metrics\n",
    "#clf.fit(X_train, y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "#metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cross_val_score(clf, X_train, y_train, cv=10)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.6\n",
    "\n",
    "Find the best parameters of the RandomForestClassifier (max_depth, max_features, n_estimators)\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_range = range(10, 310, 10)\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "# use 10-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",
    "for estimator in estimator_range:\n",
    "    clf = RandomForestClassifier(n_estimators=estimator, random_state=1, n_jobs=-1)\n",
    "    accuracy_scores.append(cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(estimator_range, accuracy_scores)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = range(1, len(feature_cols)+1)\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "# use 10-fold cross-validation with each value of max_features (WARNING: SLOW!)\n",
    "for feature in feature_range:\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_features=feature, random_state=1, n_jobs=-1)\n",
    "    accuracy_scores.append(cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Modelo de Random Forest ajustado a los mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features= XX & n_estimators= XX \n",
    "clf = RandomForestClassifier(n_estimators=200, max_features=6, random_state=1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':clf.feature_importances_}).sort_values('importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.7 \n",
    "\n",
    "Using xgboost train a XGBClassifier \n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-win_amd64.whl (97.8 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\57310\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\57310\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:24:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8989412897016361, 0.8790322580645161)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8790322580645161\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "acc= metrics.accuracy_score(y_pred, y_test.values)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.8\n",
    "\n",
    "Using xgboost train a XGBClassifier \n",
    "\n",
    "Modify the parameters learning rate, gamma, colsample_bytree. Explain what each parameter means.\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning_rate: Se utiliza para evitar el over-fitting, Después de cada boosting,se reducen los pesos de las nuevas funciones para que el proceso sea más conservador.\n",
    "* Gamma: Es la reducción de pérdida mínima requerida para hacer una partición adicional en un nodo hoja del árbol. Cuanto mayor sea gamma, más conservador será el algoritmo.\n",
    "* Colsample_bytree: es la proporción de submuestra de columnas al construir cada árbol. El submuestreo ocurre una vez por cada árbol construido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se establece la grilla de parámetros a calibrar\n",
    "params = {\n",
    "        'learning_rate': [0.08,0.09, 0.1, 0.11,0.12],\n",
    "        'gamma': [0.5,0.7,0.9,1,1.1,1,2,1,3, 1.5, 2, 5],\n",
    "        'colsample_bytree': [0.75, 0.8,0.85,0.9, 1.0],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective='binary:logistic',\n",
    "                    silent=True, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se incluye función de timer para saber cuánto se esta demorando la ejecución\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "[11:34:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:34:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 8.9 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Aquí se configuran los parámetros establecidos para la búsqueda de grilla, y se usa el auc como medida para determinar el rendimeinto de cada iteración\n",
    "\n",
    "#las métricas folds, param_comb y n_jobs están bajitas para disminuir el tiempo de ejecución, para una mejor calibración se puede aumentar el número\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "param_comb = 7\n",
    "\n",
    "\n",
    "folds=10\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=10, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "#Timer y random search\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X_train, y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([1.28365331, 1.32155859, 1.12724202, 1.05133677, 1.18740382,\n",
      "       1.00391269, 1.01555536]), 'std_fit_time': array([0.2461164 , 0.25237568, 0.22098507, 0.21371132, 0.21011206,\n",
      "       0.20090669, 0.16844218]), 'mean_score_time': array([0.00647888, 0.00577562, 0.00542638, 0.01028583, 0.00851111,\n",
      "       0.01020305, 0.00603106]), 'std_score_time': array([0.00067994, 0.00052254, 0.00042441, 0.00752354, 0.00504108,\n",
      "       0.00725929, 0.00091901]), 'param_learning_rate': masked_array(data=[0.08, 0.09, 0.1, 0.12, 0.08, 0.11, 0.09],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.9, 5, 5, 0.7, 5, 1.5, 1],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_colsample_bytree': masked_array(data=[1.0, 1.0, 0.9, 0.85, 0.8, 0.75, 0.75],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.08, 'gamma': 0.9, 'colsample_bytree': 1.0}, {'learning_rate': 0.09, 'gamma': 5, 'colsample_bytree': 1.0}, {'learning_rate': 0.1, 'gamma': 5, 'colsample_bytree': 0.9}, {'learning_rate': 0.12, 'gamma': 0.7, 'colsample_bytree': 0.85}, {'learning_rate': 0.08, 'gamma': 5, 'colsample_bytree': 0.8}, {'learning_rate': 0.11, 'gamma': 1.5, 'colsample_bytree': 0.75}, {'learning_rate': 0.09, 'gamma': 1, 'colsample_bytree': 0.75}], 'split0_test_score': array([0.89103292, 0.89330306, 0.89443814, 0.89557321, 0.89670829,\n",
      "       0.89557321, 0.89103292]), 'split1_test_score': array([0.88535755, 0.88195233, 0.87968218, 0.87968218, 0.87854711,\n",
      "       0.87968218, 0.88195233]), 'split2_test_score': array([0.88649262, 0.88535755, 0.88876277, 0.89103292, 0.8876277 ,\n",
      "       0.88649262, 0.8876277 ]), 'split3_test_score': array([0.87514188, 0.87514188, 0.87514188, 0.87173666, 0.87514188,\n",
      "       0.87173666, 0.87400681]), 'split4_test_score': array([0.88081725, 0.87627696, 0.87854711, 0.87854711, 0.87741203,\n",
      "       0.87627696, 0.87400681]), 'split5_test_score': array([0.8876277 , 0.89103292, 0.88989784, 0.88876277, 0.88989784,\n",
      "       0.88422247, 0.88649262]), 'split6_test_score': array([0.8830874 , 0.88195233, 0.87968218, 0.88649262, 0.88081725,\n",
      "       0.8876277 , 0.88876277]), 'split7_test_score': array([0.89103292, 0.88649262, 0.88535755, 0.89103292, 0.88649262,\n",
      "       0.88649262, 0.8876277 ]), 'split8_test_score': array([0.88195233, 0.87854711, 0.8830874 , 0.8830874 , 0.8830874 ,\n",
      "       0.88195233, 0.88422247]), 'split9_test_score': array([0.87968218, 0.8830874 , 0.88081725, 0.87968218, 0.88195233,\n",
      "       0.87968218, 0.87741203]), 'mean_test_score': array([0.88422247, 0.88331442, 0.88354143, 0.884563  , 0.88376844,\n",
      "       0.88297389, 0.88331442]), 'std_test_score': array([0.00481571, 0.0056252 , 0.00567991, 0.00690532, 0.00617964,\n",
      "       0.00632899, 0.00589365]), 'rank_test_score': array([2, 5, 4, 1, 3, 7, 5])}\n",
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.85, gamma=0.7, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.12, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              silent=True, subsample=1, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "\n",
      " Best normalized gini score for 10-fold search with 7 parameter combinations:\n",
      "0.7691259931895573\n",
      "\n",
      " Best hyperparameters:\n",
      "{'learning_rate': 0.12, 'gamma': 0.7, 'colsample_bytree': 0.85}\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       1.283653      0.246116         0.006479        0.000680   \n",
      "1       1.321559      0.252376         0.005776        0.000523   \n",
      "2       1.127242      0.220985         0.005426        0.000424   \n",
      "3       1.051337      0.213711         0.010286        0.007524   \n",
      "4       1.187404      0.210112         0.008511        0.005041   \n",
      "5       1.003913      0.200907         0.010203        0.007259   \n",
      "6       1.015555      0.168442         0.006031        0.000919   \n",
      "\n",
      "  param_learning_rate param_gamma param_colsample_bytree  \\\n",
      "0                0.08         0.9                    1.0   \n",
      "1                0.09           5                    1.0   \n",
      "2                 0.1           5                    0.9   \n",
      "3                0.12         0.7                   0.85   \n",
      "4                0.08           5                    0.8   \n",
      "5                0.11         1.5                   0.75   \n",
      "6                0.09           1                   0.75   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'learning_rate': 0.08, 'gamma': 0.9, 'colsamp...           0.891033   \n",
      "1  {'learning_rate': 0.09, 'gamma': 5, 'colsample...           0.893303   \n",
      "2  {'learning_rate': 0.1, 'gamma': 5, 'colsample_...           0.894438   \n",
      "3  {'learning_rate': 0.12, 'gamma': 0.7, 'colsamp...           0.895573   \n",
      "4  {'learning_rate': 0.08, 'gamma': 5, 'colsample...           0.896708   \n",
      "5  {'learning_rate': 0.11, 'gamma': 1.5, 'colsamp...           0.895573   \n",
      "6  {'learning_rate': 0.09, 'gamma': 1, 'colsample...           0.891033   \n",
      "\n",
      "   split1_test_score  ...  split3_test_score  split4_test_score  \\\n",
      "0           0.885358  ...           0.875142           0.880817   \n",
      "1           0.881952  ...           0.875142           0.876277   \n",
      "2           0.879682  ...           0.875142           0.878547   \n",
      "3           0.879682  ...           0.871737           0.878547   \n",
      "4           0.878547  ...           0.875142           0.877412   \n",
      "5           0.879682  ...           0.871737           0.876277   \n",
      "6           0.881952  ...           0.874007           0.874007   \n",
      "\n",
      "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
      "0           0.887628           0.883087           0.891033           0.881952   \n",
      "1           0.891033           0.881952           0.886493           0.878547   \n",
      "2           0.889898           0.879682           0.885358           0.883087   \n",
      "3           0.888763           0.886493           0.891033           0.883087   \n",
      "4           0.889898           0.880817           0.886493           0.883087   \n",
      "5           0.884222           0.887628           0.886493           0.881952   \n",
      "6           0.886493           0.888763           0.887628           0.884222   \n",
      "\n",
      "   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.879682         0.884222        0.004816                2  \n",
      "1           0.883087         0.883314        0.005625                5  \n",
      "2           0.880817         0.883541        0.005680                4  \n",
      "3           0.879682         0.884563        0.006905                1  \n",
      "4           0.881952         0.883768        0.006180                3  \n",
      "5           0.879682         0.882974        0.006329                7  \n",
      "6           0.877412         0.883314        0.005894                5  \n",
      "\n",
      "[7 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#Impresión de resultados\n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuta el modelo con los parámetros calibrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(learning_rate=0.12,gamma=0.7,colsamble_bytree=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:35:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"colsamble_bytree\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:35:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8820276497695853"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2.fit(X_train, y_train)\n",
    "y_pred = xgb2.predict(X_test)\n",
    "metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "El accuracy del modelo calibrado es de 88.2% siendo mejor que el del modelo sin calibrar que cuenta con un accuracy de 87.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
